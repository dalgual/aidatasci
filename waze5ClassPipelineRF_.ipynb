{
  "metadata": {
    "name": "waze5ClassPipelineRF",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dalgual/aidatasci/blob/main/waze5ClassPipelineRF_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilnu-YdmV0u_"
      },
      "source": [
        "Author: Jongwook Woo, Dalya Manatova\n",
        "Created at 04/07/2023\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_L59P3GV0vB"
      },
      "source": [
        "# Waze Traffic Data Classification with Random Forrest __PySpark__ with GPU\n",
        "# Multiclassification\n",
        "\n",
        "Dense of Waze Traffic is an example of xgboost classifier to do binary/mulit-class classification. This notebook will show you how to load data, train the xgboost model and use this model to predict if a location is dense or not.\n",
        "\n",
        "## Load libraries\n",
        "First load some common libraries will be used by both GPU version and CPU version xgboost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "S074nwiHV0vB"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "from ml.dmlc.xgboost4j.scala.spark import XGBoostClassificationModel, XGBoostClassifier\n",
        "#from ml.dmlc.xgboost4j.scala.spark.rapids import GpuDataReader\n",
        "from ml.dmlc.xgboost4j.scala.spark import XGBoostRegressionModel, XGBoostRegressor\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
        "from pyspark.ml.feature import StringIndexer, VectorIndexer, MinMaxScaler, SQLTransformer, Normalizer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\n",
        "\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics # # performance metrics\n",
        "from pyspark.mllib.evaluation import MultilabelMetrics\n",
        "\n",
        "from pyspark.storagelevel import StorageLevel\n",
        "\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "pCUOTBWYV0vD"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "\n",
        "from time import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqpMg9BoV0vD"
      },
      "source": [
        "## Build the schema and parameters\n",
        "The mortgage data has 27 columns: 26 features and 1 label. \"deinquency_12\" is the label column. The schema will be used to load data in the future.\n",
        "\n",
        "The next block also defines some key parameters used in xgboost training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "0HpagKimV0vD"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "IS_CPU = True # False\n",
        "IS_SAMPLE = False #True #False\n",
        "IS_HALF = False #False\n",
        "IS_SHOW = True\n",
        "\n",
        "# TSV or CV?\n",
        "IS_TSV = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW0I9qXEV0vE"
      },
      "source": [
        "### Schema\n",
        "\"location_x\", \"location_y\", \"sin_weekday\", \"cos_weekday\", \"sin_month\", \"cos_month\", \"sin_day\", \"cos_day\", \"sin_hour\", \"cos_hour\", \"sin_min\", \"cos_min\", \"sin_sec\", \"cos_sec\", \"is_rush\", \"is_weekend\", \"is_holiday\", \"level\", \"trueLabel\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "Ur2rep3AV0vE"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "#labelColName = \"trueLevel\"\n",
        "labelColName = \"trueLabel\"\n",
        "\n",
        "schema = StructType([\n",
        "  StructField(\"location_x\", DoubleType()),\n",
        "  StructField(\"location_y\", DoubleType()),\n",
        "  StructField(\"sin_weekday\", DoubleType()),\n",
        "  StructField(\"cos_weekday\", DoubleType()),\n",
        "  StructField(\"sin_month\", DoubleType()),\n",
        "  StructField(\"cos_month\", DoubleType()),\n",
        "  StructField(\"sin_day\", DoubleType()),\n",
        "  StructField(\"cos_day\", DoubleType()),\n",
        "  StructField(\"sin_hour\", DoubleType()),\n",
        "  StructField(\"cos_hour\", DoubleType()),\n",
        "  StructField(\"sin_min\", DoubleType()),\n",
        "  StructField(\"cos_min\", DoubleType()),\n",
        "  StructField(\"sin_sec\", DoubleType()),\n",
        "  StructField(\"cos_sec\", DoubleType()),\n",
        "  StructField(\"is_rush\", IntegerType()),\n",
        "  StructField(\"is_weekend\", IntegerType()),\n",
        "  StructField(\"is_holiday\", IntegerType()),\n",
        "  StructField(\"level\", IntegerType())])\n",
        "#  StructField(\"label\", IntegerType())])\n",
        "\n",
        "\n",
        "featureNames = [ x.name for x in schema if x.name != \"level\" ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "v3EjyBo8V0vE"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\n",
        "LOG_LEVEL = \"OFF\" #\"INFO\"\n",
        "spark.sparkContext.setLogLevel(LOG_LEVEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "ogtXpr4SV0vE"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# CSV options\n",
        "infer_schema = \"true\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "0QJo_6RgV0vF"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "file_location = \"/user/hadoop/waze/\"\n",
        "dir = \"mid_data\"\n",
        "file_type = \"csv\"\n",
        "if (IS_SAMPLE == True):\n",
        "    dir = \"sample_mid_data\"\n",
        "    file_type = \"csv\"\n",
        "elif (IS_HALF == True):\n",
        "    dir = \"mid_half_data\"\n",
        "    file_type = \"csv\" #\"parquet\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "DMtmuEuqV0vF"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "'''dataTrans = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(file_location + dir)\n",
        "'''\n",
        "dataTrans = spark.read.schema(schema) \\\n",
        "    .option('header', first_row_is_header) \\\n",
        "    .csv(file_location + dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "5__1CZaLV0vF"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "temp_table_name = \"jampredictclean_1m_100mb_csv\"\n",
        "dataTrans.createOrReplaceTempView(temp_table_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "gCJ7DGe4V0vF"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "dataTrans.persist(StorageLevel.MEMORY_AND_DISK )\n",
        "dataTrans.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "EVKWI2EZV0vF"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "if(IS_HALF == True):\n",
        "    dataTrans = dataTrans.sample(0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "KaWXo1h9V0vF"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "splits = dataTrans.randomSplit([0.75, 0.25])\n",
        "# for decision tree classifier\n",
        "train = splits[0].withColumnRenamed(\"level\", \"label\")\n",
        "test = splits[1].withColumnRenamed(\"level\", labelColName)\n",
        "\n",
        "print (\"Training Rows:\", train.count(), \" Testing Rows:\", test.count())\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %pyspark\n",
        "if IS_SHOW == True:\n",
        "    train.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "61vku2dSV0vF"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "trainSet = train\n",
        "evalSet  = test\n",
        "transSet = test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "cL0boysuV0vF"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "if IS_SHOW == True: trainSet.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "fhlB6ZDaV0vF"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "if IS_SHOW == True: transSet.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "BnkgmHjhV0vF"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "featureNames = [ x.name for x in schema if x.name != \"label\" ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "wgzFViRHV0vF"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "if IS_SHOW == True: print(\"schema\", featureNames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "NRVRqFC9V0vG"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# For finite numeric valuse\n",
        "catVect = VectorAssembler(inputCols = [\"sin_weekday\", \"cos_weekday\", \"sin_month\", \"cos_month\", \"sin_day\", \"cos_day\", \"sin_hour\", \"cos_hour\", \"sin_min\", \"cos_min\", \"sin_sec\", \"cos_sec\", \"is_rush\", \"is_weekend\", \"is_holiday\"], outputCol=\"catFeatures\") #.setHandleInvalid(\"skip\")\n",
        "catIdx = VectorIndexer(inputCol = catVect.getOutputCol(), outputCol = \"idxCatFeatures\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "f-OyEUBJV0vG"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# infinite number is meaningful so that it should be number features\n",
        "numVect = VectorAssembler(inputCols = ['location_x', 'location_y'], outputCol=\"numFeatures\")\n",
        "# number vector is normalized\n",
        "minMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol=\"normFeatures\")\n",
        "\n",
        "# combine finite and infinite numeric list\n",
        "featVect = VectorAssembler(inputCols=[\"idxCatFeatures\", \"normFeatures\"], outputCol=\"features\")  #=\"features1\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "zc12RVyjV0vG"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# NOTE: labelCol=\"label\" not \"trueLabel\"\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
        "\n",
        "# pipeline = Pipeline(stages=[assembler, rf])\n",
        "pipeline = Pipeline(stages=[catVect, catIdx, numVect, minMax, featVect, rf])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "uWEfcGMxV0vG"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "            .addGrid(rf.numTrees, [10, 20]) \\\n",
        "            .addGrid(rf.minInfoGain, [0.0, 0.01]) \\\n",
        "            .addGrid(rf.maxDepth, [5, 10]).build()\n",
        "# TODO: K = 2, you may test it with 5, 10\n",
        "# K=2, 3, 5, 10: Root Mean Square Error (RMSE): 13.2\n",
        "K = 2\n",
        "if(IS_TSV == True):\n",
        "    cv = TrainValidationSplit(estimator=pipeline, evaluator=MulticlassClassificationEvaluator(),  estimatorParamMaps=paramGrid, trainRatio=0.8)\n",
        "else:\n",
        "    cv = CrossValidator(estimator=pipeline, evaluator=MulticlassClassificationEvaluator(), estimatorParamMaps=paramGrid, numFolds=K)\n",
        "\n",
        "model = cv.fit(trainSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "ZWBzjlf4V0vG"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "'''\n",
        "rf2 = RandomForestClassifier(labelCol=\"level\", featuresCol=\"features\")\n",
        "pipeline = Pipeline(stages=[assembler, rf2])\n",
        "model = pipeline.fit(trainSet)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "Vp89FQApV0vG"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "#from time import time\n",
        "'''def with_benchmark(phrase, action):\n",
        "    start = time()\n",
        "    result = action()\n",
        "    end = time()\n",
        "    print('{} takes {} seconds'.format(phrase, round(end - start, 2)))\n",
        "    return result\n",
        "\n",
        "model = with_benchmark('Training', lambda: cv.fit(trainSet))'''\n",
        "\n",
        "'''start = time()\n",
        "model = cv.fit(trainSet)\n",
        "end = time()\n",
        "print('{} takes {} seconds'.format('Training', round(end - start, 2)))'''"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain",
        "id": "v-SruM-JV0vG"
      },
      "source": [
        "%pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "oPP4hTI9V0vG"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "def transform():\n",
        "    result = model.transform(transSet).cache()\n",
        "    result.foreachPartition(lambda _: None)\n",
        "    return result\n",
        "\n",
        "#predicted = with_benchmark('Transformation', transform)\n",
        "\n",
        "result = model.transform(transSet)\n",
        "predicted = result\n",
        "predicted.select(labelColName, 'rawPrediction', 'probability', 'prediction').show(5)\n",
        "\n",
        "\n",
        "'''prediction = model.transform(test)\n",
        "#predicted = prediction.select(\"features\", \"prediction\", \"trueLabel\")\n",
        "predicted = prediction.select(\"normFeatures\", \"prediction\", \"trueLabel\")'''\n",
        "\n",
        "predicted.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "SkDFwHM7V0vG"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "'''accuracy = with_benchmark(\n",
        "    'Evaluation',\n",
        "    lambda: MulticlassClassificationEvaluator().setLabelCol(labelColName).evaluate(predicted))\n",
        "'''\n",
        "accuracy = MulticlassClassificationEvaluator().setLabelCol(labelColName).evaluate(predicted)\n",
        "\n",
        "print('Accuracy is ' + str(accuracy))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "O2D_vdAMV0vG"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol =labelColName, predictionCol=\"prediction\")\n",
        "print(\"Test-set Accuracy is : \", evaluator.evaluate(predicted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "b4LSYdAEV0vG"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "#important: need to cast to float type, and order by prediction, else it won't work\n",
        "preds_and_labels = predicted.select(['prediction',labelColName])\\\n",
        "                              .withColumn(labelColName, col(labelColName)\\\n",
        "                              .cast(FloatType()))\\\n",
        "                              .orderBy('prediction')\n",
        "\n",
        "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
        "\n",
        "print(metrics.confusionMatrix().toArray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "dmSywkjEV0vG"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "labels = train.rdd.map(lambda lp: lp.label).distinct().collect()\n",
        "for label in sorted(labels):\n",
        "    print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
        "    print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
        "    #print(\"Class %s F1 Measure = %s\" % (label, metrics.fMeasure(label, beta=1.0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "TzHjLSUxV0vG"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "accuracy = metrics.accuracy\n",
        "precision1 = metrics.precision(1)\n",
        "precision2 = metrics.precision(2)\n",
        "precision3 = metrics.precision(3)\n",
        "precision4 = metrics.precision(4)\n",
        "precision5 = metrics.precision(5)\n",
        "\n",
        "print (\"accuracy = \", accuracy)\n",
        "print (\" precision1 = \", precision1, \" precision2 = \", precision2,  \" precision3 = \", precision3, \" precision4 = \", precision4, \" precision5 = \", precision5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "1qRchZohV0vG"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "recall1 = metrics.recall(1)\n",
        "recall2 = metrics.recall(2)\n",
        "recall3 = metrics.recall(3)\n",
        "recall4 = metrics.recall(4)\n",
        "recall5 = metrics.recall(5)\n",
        "\n",
        "print (\" recall1 = \", recall1, \" recall2 = \", recall2,  \" recall3 = \", recall3, \" recall4 = \", recall4, \" recall5 = \", recall5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "wEakijwQV0vH"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# Weighted stats\n",
        "print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
        "print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
        "print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
        "print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
        "print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "WOuMnKJ3V0vH"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels = [1, 2, 3, 4, 5]\n",
        "_ = plt.figure(figsize=(7, 7))\n",
        "sns.heatmap(metrics.confusionMatrix().toArray(),\n",
        "            cmap='viridis',\n",
        "            annot=True,fmt='0',\n",
        "            cbar=False,\n",
        "            xticklabels=labels,\n",
        "            yticklabels=labels)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "I8dVXrtyV0vH"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "\n",
        "'''z.show(results.select(\"trueLabel\", labelColName,\"rawPrediction\",\"probability\",\"prediction\").limit(10))\n",
        "display(pipelineModel.stages[-1], predDF.drop(\"prediction\", \"rawPrediction\", \"probability\"), \"ROC\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt54U_lHV0vH"
      },
      "source": [
        "__References__\n",
        "1. Multiclass Classification, https://spark.apache.org/docs/3.3.0/mllib-evaluation-metrics.html#multiclass-classification\n",
        "1. https://github.com/NVIDIA/spark-rapids-examples/blob/main/examples/XGBoost-Examples/mortgage/notebooks/scala/mortgage_gpu_crossvalidation.ipynb\n",
        "1. MultiClass Classification using PySPark, https://www.kaggle.com/code/ashokkumarpalivela/multiclass-classification-using-pyspark\n",
        "1. XGBoost Distributed Training and Parallel Predictions with Apache Spark, https://medium.com/cloudzone/xgboost-distributed-training-and-predicting-with-apache-spark-1127cdfb31ae\n",
        "1. Use XGBoost on Databricks, https://docs.databricks.com/machine-learning/train-model/xgboost.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6UREaVgV0vI"
      },
      "source": []
    }
  ]
}