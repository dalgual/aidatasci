{
  "metadata": {
    "name": "airbnbPrice_XGBoost_emr",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "colab": {
      "name": "airbnbPrice_XGBoost_emr..ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dalgual/aidatasci/blob/main/airbnbPrice_XGBoost_emr_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a4vVRGVZBA7"
      },
      "source": [
        "\n",
        "------\n",
        "#### Authors: Samyuktha Muralidharan\n",
        "\n",
        "#### Instructor: [Jongwook Woo](https://www.linkedin.com/in/jongwook-woo-7081a85)\n",
        "\n",
        "#### Date: 05/23/2021\n",
        "#### Updated: 12/22/2021 for AWS, Savita Yadav & Jwoo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6XxKl-PZBA8"
      },
      "source": [
        "### References\n",
        "1. https://github.com/rapidsai/spark-examples/blob/master/examples/notebooks/python/mortgage-gpu.ipynb\n",
        "1. https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-rapids.html\n",
        "1. https://docs.databricks.com/_static/notebooks/xgboost-regression.html\n",
        "1. https://docs.databricks.com/_static/notebooks/xgboost-pyspark.html\n",
        "1. https://towardsdatascience.com/pyspark-and-xgboost-integration-tested-on-the-kaggle-titanic-dataset-4e75a568bdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulKr0QUbZBA9"
      },
      "source": [
        "\n",
        "## Objective\n",
        "**Airbnb** is an online marketplace that connects people who want to rent out their homes with people looking for accommodations in that locale. \n",
        "The Aim is to understand how Airbnb hosts can make simple changes to their properties to boost customer satisfaction.  We used a **Classification model** for Rating Prediction, and the algorithm used here is **Decision Tree Classifier**. This experiment predicts Customer's behavior in classifying their reviews as high rated or low rated using feature Review Score Rating."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGbd7HzkZBA9"
      },
      "source": [
        "\n",
        "## Import Spark SQL and Spark ML Libraries\n",
        "Import all the Spark SQL and ML libraries as mentioned below. This is neccessary to access the functions available in those libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "KxL31jTlZBA-"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# Import Spark SQL and Spark ML libraries\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "from pyspark.storagelevel import StorageLevel\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler,StringIndexer, VectorIndexer, MinMaxScaler\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, TrainValidationSplit\n",
        "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator\n",
        "from pyspark.ml.classification import LogisticRegression,DecisionTreeClassifier\n",
        "\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "\n",
        "from ml.dmlc.xgboost4j.scala.spark import XGBoostClassificationModel, XGBoostClassifier\n",
        "#from ml.dmlc.xgboost4j.scala.spark.rapids import GpuDataReader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHrZbvdMZBBA"
      },
      "source": [
        "\n",
        "##Read csv file from DBFS  (Databricks File System)\n",
        "1. After <filename>.csv file is added to the data in the left frame, create a table using the UI, \"Upload File\"\n",
        "2. Click \"Preview Table to view the table\" and select the option as <filename>.csv has a header as the first row. \"First line is header\"\n",
        "3. Change the data type of the columns\n",
        "4. Click on the create table button.\n",
        "  \n",
        " The link to the sampled file : https://www.kaggle.com/samyukthamurali/airbnb-ratings-dataset?select=airbnb_sample.csv. You can download the sample file from here and upload it in DBFS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "YkMI9hyhZBBA"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "start = time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "qfjBZIOrZBBB"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# from ingest.Connectors import Connectors\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "IS_AWS = False #True #\n",
        "IS_CPU = True # False #True #  \n",
        "# airbnb-listings.csv: seperated by ;\n",
        "#   airbnb_sample.csv and airbnb_US.csv: seperated by ,\n",
        "IS_FULL_DATA = True #False #  \n",
        "\n",
        "AWS_BUCKET_NAME = \"hipicdatasets\"\n",
        "MOUNT_NAME = \"airbnb\"  # mounted name\n",
        "  \n",
        "#aws_bucket_name = \"cis5560\"\n",
        "mount_name = \"airbnb\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "etjHz80aZBBC"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# File location and type: file_path = 'gs://hdp-240/airbnb/airbnb_US.csvv'\n",
        "if (IS_AWS == True):\n",
        "    file_location = \"s3://bigdai-pub/airbnb_listings.csv\" # 1.93 GB\n",
        "    #file_location = \"s3://hipicdatasets/airbnb-listings.csv\" # 1.92 GB\n",
        "    #file_location = \"s3://hipicdatasets/airbnb_sample.csv\" # 32MB\n",
        "    # file_location = \"s3://hipicdatasets/airbnb_US.csv\" # 370.5 MB\n",
        "else: \n",
        "    #GCP:\n",
        "    #file_location = \"gs://hdp-240/airbnb/airbnb_US.csv\"\n",
        "    #file_location = \"gs://hdp-240/airbnb/airbnb_sample.csv\"\n",
        "    # HDFS\n",
        "    file_location = \"/user/hadoop/airbnb/airbnb_listings.csv\"\n",
        "\n",
        "\n",
        "file_type = \"csv\"\n",
        "\n",
        "# CSV options\n",
        "infer_schema = \"true\"\n",
        "first_row_is_header = \"true\"\n",
        "if (IS_FULL_DATA == True):\n",
        "    delimiter = \";\" #\",\"\n",
        "else:\n",
        "    delimiter = \",\"\n",
        "\n",
        "# The applied options are for CSV files. For other file types, these will be ignored.\n",
        "# Load the csv file as a pyspark dataframe\n",
        "df = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(file_location)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "a3EtLdjTZBBD"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "end = time()\n",
        "phrase=\"Data Reading Time\"\n",
        "print('{} takes {} seconds'.format(phrase, (end - start))) #round(end - start, 2)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "oV4bM_iNZBBD"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8hKAwgrZBBD"
      },
      "source": [
        "\n",
        "## Create a temporary view of the dataframe 'df'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "l0f4SQrBZBBD"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# Create a view or table\n",
        "temp_table_name = \"airbnb_sample_csv\"\n",
        "#df.createOrReplaceTempView(temp_table_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5et_ieO3ZBBE"
      },
      "source": [
        "\n",
        "## Create a dataframe from the table, using Spark SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "deBU0luIZBBE"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "#csv = spark.sql(\"SELECT * FROM airbnb_sample_csv\")\n",
        "#csv = df.select(\"Review Scores Rating\", \"Host Listings Count\", \"Host Total Listings Count\", \"Calculated host listings count\", \"Security Deposit\", \"Cleaning Fee\" , \"Host Response Time\",\"Host Response Rate\",\"Host Acceptance Rate\",\"Property Type\",\"Room Type\",\"Price\",\"Weekly Price\",\"Monthly Price\",\"Maximum Nights\",\"Review Scores Accuracy\",\"Review Scores Cleanliness\",\"Review Scores Checkin\",\"Review Scores Communication\",\"Review Scores Location\",\"Review Scores Value\",\"Cancellation Policy\",\"Bedrooms\",\"Bathrooms\",\"Beds\",\"Extra People\",\"Minimum Nights\")\n",
        "\n",
        "#csv.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "-Jv7FprpZBBE"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "phrase= \"data engineering time: \"\n",
        "start = time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "nb4mgn7DZBBE"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "csv = csv.withColumn(\"review_scores_rating\", when(col(\"review_scores_rating\") >= 80,1).otherwise(0))\n",
        "csv = csv.withColumn(\"host_response_rate\", csv[\"host_response_rate\"].cast(IntegerType()))\n",
        "csv = csv.withColumn(\"host_listings_count\", csv[\"host_listings_count\"].cast(IntegerType()))\n",
        "csv = csv.withColumn(\"host_total_listings_count\", csv[\"host_total_listings_count\"].cast(IntegerType()))\n",
        "csv = csv.withColumn(\"price\", csv[\"price\"].cast(IntegerType()))\n",
        "csv = csv.withColumn(\"weekly_price\", csv[\"weekly_price\"].cast(IntegerType()))\n",
        "csv = csv.withColumn(\"monthly_price\", csv[\"monthly_price\"].cast(IntegerType()))\n",
        "\n",
        "csv = csv.withColumn(\"maximum_nights\", csv[\"maximum_nights\"].cast(IntegerType()))\n",
        "csv = csv.withColumn(\"review_scores_accuracy\", csv[\"review_scores_accuracy\"].cast(IntegerType()))\n",
        "csv = csv.withColumn(\"review_scores_cleanliness\", csv[\"review_scores_cleanliness\"].cast(IntegerType()))\n",
        "csv = csv.withColumn(\"review_scores_checkin\", csv[\"review_scores_checkin\"].cast(IntegerType()))\n",
        "csv = csv.withColumn(\"review_scores_communication\", csv[\"review_scores_communication\"].cast(IntegerType()))\n",
        "csv = csv.withColumn(\"review_scores_location\", csv[\"review_scores_location\"].cast(IntegerType()))\n",
        "\n",
        "csv = csv.withColumn(\"review_scores_value\", csv[\"review_scores_value\"].cast(IntegerType()))\n",
        "csv = csv.withColumn(\"calculated_host_listings_count\", csv[\"calculated_host_listings_count\"].cast(IntegerType()))\n",
        "csv = csv.withColumn(\"bedrooms\", csv[\"bedrooms\"].cast(IntegerType()))\n",
        "csv = csv.withColumn(\"bathrooms\", csv[\"bathrooms\"].cast(IntegerType()))\n",
        "csv = csv.withColumn(\"beds\", csv[\"beds\"].cast(IntegerType()))\n",
        "csv = csv.withColumn(\"security_deposit\", csv[\"security_deposit\"].cast(IntegerType()))\n",
        "\n",
        "csv = csv.withColumn(\"host_acceptance_rate\", csv[\"host_acceptance_rate\"].cast(IntegerType()))\n",
        "csv = csv.withColumn(\"cleaning_fee\", csv[\"cleaning_fee\"].cast(IntegerType()))\n",
        "csv = csv.withColumn(\"extra_people\", csv[\"extra_people\"].cast(IntegerType()))\n",
        "csv = csv.withColumn(\"minimum_nights\", csv[\"minimum_nights\"].cast(IntegerType()))\n",
        "\n",
        "csv.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "328Q1WMyZBBF"
      },
      "source": [
        "\n",
        "## Selecting Columns\n",
        "In the following step, we are selecting the columns that are useful for Rating Prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "ZSaP4pRfZBBF"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "csv = csv.filter(col(\"minimum_nights\")<= 365)\n",
        "\n",
        "data = csv.select(\"review_scores_rating\", \"host_listings_count\", \"host_total_listings_count\", \"calculated_host_listings_count\", \"security_deposit\", \"cleaning_fee\" , \"host_response_time\",\"host_response_rate\",\"host_acceptance_rate\",\"property_type\",\"room_type\",\"bed_type\", \"weekly_price\",\"monthly_price\",\"maximum_nights\",\"review_scores_accuracy\",\"review_scores_cleanliness\",\"review_scores_checkin\",\"review_scores_communication\",\"review_scores_location\",\"review_scores_value\",\"cancellation_policy\",\"bedrooms\",\"bathrooms\",\"beds\",\"extra_people\",\"minimum_nights\",col(\"price\").cast(\"Int\").alias(\"label\"))\n",
        "\n",
        "data.show(5)\n",
        "\n",
        "#display(data.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8Kc40eTZBBF"
      },
      "source": [
        "\n",
        "## Data Cleaning\n",
        "**Handling Missing Values:** Filling the missing values of numeric columns with **'0'** and string columns with **'NA'**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "jMJuZmK3ZBBG"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# Filter Property Type not in the correct list\n",
        "property_list = [\"Apartment\",\"House\",\"Bed & Breakfast\",\"Condominium\",\"Loft\", \"Townhouse\",\"Other\",\"Villa\", \"Guesthouse\", \"Bungalow\", \"Dorm\", \"Boat\", \"Cabin\", \"Chalet\", \"Boutique hotel\", \"Serviced apartment\", \"Hostel\", \"Camper/RV\", \"Timeshare\", \"Guest suite\", \"Tent\", \"Vacation home\", \"Castle, Treehouse\", \"In-law\", \"Earth House\", \"Hut\", \"Yurt\", \"Entire Floor\", \"Tipi\", \"Nature lodge\", \"Cave\", \"Lighthouse\", \"Casa particular\", \"Train\", \"Island\", \"Igloo\", \"Parking Space\", \"Pension (Korea)\", \"Ryokan (Japan)\", \"Car\", \"Heritage hotel (India)\", \"Plane\", \"Van\"\n",
        "]\n",
        "\n",
        "data = data.filter(data.property_type.isin(property_list))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "XrtS2aTTZBBG"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "data.persist(StorageLevel.DISK_ONLY_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "qGptnn-lZBBG"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "outliers = data.stat.approxQuantile([\"bathrooms\",\"bedrooms\",\"monthly_price\",\"extra_people\",\"minimum_nights\",\"label\"], [0.05,0.95],0.0)\n",
        " \n",
        "print(outliers)\n",
        " \n",
        "#Filtering the dataframe by removing the outliers\n",
        "#  data1 = data.filter(col(\"Host Listings Count\") >= outliers(0)(0) && col(\"Host Listings Count\")  <= outliers(0)(1))\n",
        "#  data2 = data.filter(col(\"Host Total Listings Count\") >= outliers(1)(0) && col(\"Host Total Listings Count\")  <= outliers(1)(1))\n",
        "# data3 = data.filter(data[\"Accommodates\"] >= outliers[0][0] and data[\"Accommodates\"]  <= outliers[0][1])\n",
        "data4 = data.filter((data[\"bathrooms\"] >= outliers[0][0]) & (data[\"bathrooms\"]  <= outliers[0][1]))\n",
        "data5 = data4.filter((data4[\"bedrooms\"] >= outliers[1][0]) & (data4[\"bedrooms\"]  <= outliers[1][1]))\n",
        "data6 = data5.filter((data5[\"monthly_price\"] >= outliers[2][0]) & (data5[\"monthly_price\"]  <= outliers[2][1]))\n",
        "# data7 = data6.filter(data6[\"Cleaning Fee\"] >= outliers[4][0] and data6[\"Cleaning Fee\"]  <= outliers[4][1])\n",
        "#data8 = data7.filter(data7[\"Guests Included\"] >= outliers[5][0]) and data7[\"Guests Included\"]  <= outliers[5][1])\n",
        "data9 = data6.filter((data6[\"extra_people\"] >= outliers[3][0]) & (data6[\"extra_people\"]  <= outliers[3][1]))\n",
        "data10 = data9.filter((data9[\"minimum_nights\"] >= outliers[4][0]) & (data9[\"minimum_nights\"]  <= outliers[4][1]))\n",
        "\n",
        "final_data = data10.filter((data10[\"label\"] >= outliers[5][0]) & (data10[\"label\"]  <= outliers[5][1]))\n",
        "  \n",
        "final_data.show(30)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "9MPWRT15ZBBG"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "#data_clean = data.na.fill(value=0).na.fill(\"\")\n",
        "data_clean = final_data.na.fill(value=0).na.fill(\"NA\")\n",
        "data_clean.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUd7eOK_ZBBH"
      },
      "source": [
        "\n",
        "## Convert the string type columns into indices using StringIndexer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "GN4Dw63NZBBH"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# jwoo: add .setHandleInvalid(\"skip\"): or \"keep\" for null value\n",
        "data_clean = StringIndexer(inputCol='host_response_time', outputCol='host_response_time_index').setHandleInvalid(\"skip\").fit(data_clean).transform(data_clean)\n",
        "\n",
        "data_clean = StringIndexer(inputCol='cancellation_policy', outputCol='cancellation_policy_index').setHandleInvalid(\"skip\").fit(data_clean).transform(data_clean)\n",
        "\n",
        "data_clean = StringIndexer(inputCol='property_type', outputCol='property_type_index').setHandleInvalid(\"keep\").fit(data_clean).transform(data_clean)\n",
        "data_clean= StringIndexer(inputCol='room_type', outputCol='room_type_index').setHandleInvalid(\"keep\").fit(data_clean).transform(data_clean)\n",
        "#data_clean = StringIndexer(inputCol='Bed Type', outputCol='BedType_index').setHandleInvalid(\"keep\").fit(data_clean).transform(data_clean)\n",
        "#jwoo\n",
        "data_clean = StringIndexer(inputCol='bed_type', outputCol='bed_type_index').setHandleInvalid(\"keep\").fit(data_clean).transform(data_clean)\n",
        "#data_clean = StringIndexer(inputCol='Host Listings Count', outputCol='Host_Listings_Count_index').setHandleInvalid(\"keep\").fit(data_clean).transform(data_clean)\n",
        "\n",
        "data_clean = StringIndexer(inputCol=\"review_scores_rating\", outputCol='review_scores_rating_index').fit(data_clean).transform(data_clean)\n",
        "\n",
        "data_clean.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE-HPUa8ZBBH"
      },
      "source": [
        "\n",
        "## Split the data\n",
        "In the next step we split the data in a train and test set. We have split the data in the ratio of **70 to 30**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "oQb5mjS2ZBBH"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#jwoo: drop the unneccessary columns as index value generated from them\n",
        "final_df = data_clean.drop('host_response_time', \"property_type\", \"room_type\", \"bed_type\", \"cancellation_policy\", \"review_scores_rating\")\n",
        "final_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "opx9uOhrZBBH"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "final_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "CIgURPVeZBBI"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# Split the data\n",
        "splits = final_df.randomSplit([0.7, 0.3])\n",
        "\n",
        "# for decision tree classifier\n",
        "train = splits[0]\n",
        "test = splits[1].withColumnRenamed(\"label\", \"trueLabel\")\n",
        "\n",
        "print (\"Training Rows:\", train.count(), \" Testing Rows:\", test.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "8XYHoL65ZBBI"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "train.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "TA9v5kQtZBBI"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "labelColName = \"label\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "7BWqMVVKZBBI"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# features = [ x for x in train.columns if ((x != labelColName) and (x != 'host_response_time') and (x != 'cancellation_policy') and (x != 'property_type') and (x != \"room_type\") and (x != \"review_scores_rating\"))]\n",
        "features = [ x for x in train.columns]\n",
        "\n",
        "print(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2lkgrvbZBBI"
      },
      "source": [
        "\n",
        "## Create XGBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "oaTDXsM-ZBBJ"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "params = { \n",
        "    'eta': 0.1,\n",
        "    'gamma': 0.1,\n",
        "    'missing': 0.0,\n",
        "    #'treeMethod': 'gpu_hist',\n",
        "    'maxDepth': 10, \n",
        "    'maxLeaves': 256,\n",
        "    'growPolicy': 'depthwise',\n",
        "    'objective': 'reg:squarederror',\n",
        "    'minChildWeight': 30.0,\n",
        "    'lambda_': 1.0,\n",
        "    'scalePosWeight': 2.0,\n",
        "    'subsample': 1.0,\n",
        "    'nthread': 1,\n",
        "    'numRound': 100,\n",
        "    'numWorkers': 1,\n",
        "}\n",
        "\n",
        "#classifier = XGBoostClassifier(**params).setLabelCol(labelColName).setFeaturesCols(features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "JjXQ31l3ZBBJ"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "print(\"Value : %s\" %  params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "669-DCQXZBBJ"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# the nthread configuration (2) must be no larger than spark.task.cpus (1)\n",
        "if (IS_CPU == True):\n",
        "    params2 = {\"treeMethod\": \"hist\", \"numWorkers\": 1, \"nthread\": 1} # \"numWorkers\": 1, 63sec, \"numWorkers\": 2, 99sec\n",
        "else:\n",
        "    params2 = {\"treeMethod\": \"gpu_hist\", \"numWorkers\": 1, \"nthread\": 1} # \"numWorkers\": 1, 61sec\n",
        "params.update(params2) \n",
        "xgbParamFinal = params\n",
        "\n",
        "print(\"Value : %s\" %  xgbParamFinal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "zIy6fETGZBBJ"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "from ml.dmlc.xgboost4j.scala.spark import XGBoostRegressionModel, XGBoostRegressor\n",
        "\n",
        "xgBoost = XGBoostRegressor(**xgbParamFinal).setLabelCol(labelColName).setFeaturesCols(features)\n",
        "# xgBoost = XGBoostClassifier(**params).setLabelCol(labelColName).setFeaturesCols(\"features\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1OSKpROZBBK"
      },
      "source": [
        "\n",
        "## Define the Pipeline\n",
        "Define a pipeline that creates a feature vector and trains a regression model\n",
        "1. A **VectorAssembler** that combines categorical features into a single vector.\n",
        "2. A **Vector Indexer** that creates indices for a vector of categorical features.\n",
        "3. A **VectorAssembler** that creates a vector of continuous numeric features.\n",
        "4. A **MinMaxScaler** to normalize the continuous numeric features.\n",
        "5. A **VectorAssembler** that creates a vector of categorical and continuous features.\n",
        "6. A **Decision Tree Classifier** that trains a Classification model.\n",
        "7. **Process pipeline** with the series of transformations above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "vDLV9H8UZBBK"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "catVect = VectorAssembler(inputCols = [\"host_response_time_index\", \"cancellation_policy_index\", \"property_type_index\", \"room_type_index\", 'review_scores_rating_index'], outputCol=\"catFeatures\")\n",
        "\n",
        "catIdx = VectorIndexer(inputCol = catVect.getOutputCol(), outputCol = \"idxCatFeatures\").setHandleInvalid(\"skip\") \n",
        "\n",
        "#numVect = VectorAssembler(inputCols = [\"Host Response Rate\",\"Host Listings Count\",\"Host Total Listings Count\",\"Price\",\"Weekly Price\",\"Monthly Price\",\"Maximum Nights\",\"Review Scores Accuracy\",\"Review Scores Cleanliness\",\"Review Scores Checkin\",\"Review Scores Communication\",\"Review Scores Location\",\"Review Scores Value\",\"Calculated host listings count\",\"Bedrooms\",\"Bathrooms\",\"Beds\",\"Security Deposit\",\"Cleaning Fee\",\"Extra People\",\"Minimum Nights\"], outputCol=\"numFeatures\")\n",
        "\n",
        "# remove  \"Price\",\n",
        "#numVect = VectorAssembler(inputCols = [\"Host Response Rate\",\"Host Acceptance Rate\",\"Weekly Price\",\"Monthly Price\",\"Maximum Nights\",\"Review Scores Accuracy\",\"Review Scores Cleanliness\",\"Review Scores Checkin\",\"Review Scores Communication\",\"Review Scores Location\",\"Review Scores Value\",\"Bedrooms\",\"Bathrooms\",\"Beds\",\"Extra People\",\"Minimum Nights\"], outputCol=\"numFeatures\")\n",
        "numVect = VectorAssembler(inputCols = [\"host_listings_count\", \"host_total_listings_count\",\"bathrooms\",\"bedrooms\",\"monthly_price\",\"minimum_nights\",\"review_scores_accuracy\",\"review_scores_cleanliness\",\"review_scores_checkin\",\"review_scores_communication\",\"review_scores_location\",\"review_scores_value\"], outputCol=\"numFeatures\")\n",
        "minMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol=\"normFeatures\")\n",
        "\n",
        "featVect = VectorAssembler(inputCols=[\"idxCatFeatures\", \"normFeatures\"],  outputCol=\"features\")\n",
        "\n",
        "#dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
        "#xgBoost = XGBoostClassifier(labelCol=\"label\", featuresCol=\"features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "VbS8MLhFZBBK"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "pipeline = Pipeline(stages=[catVect,catIdx,numVect, minMax,featVect, xgBoost])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "ig0HZ7xOZBBK"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "end=time()\n",
        "phrase = 'data engineering time'\n",
        "print('{} takes {} seconds'.format(phrase, (end - start))) #round(end - start, 2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkB_duY7ZBBK"
      },
      "source": [
        "\n",
        "### Train a Regression model using Parameter Tuning\n",
        "Use the  **CrossValidator** class to evaluate each combination of parameters defined in a **ParameterGrid** against multiple folds of the data split into training and validation datasets, in order to find the best performing parameters. It is used to find the best model for the data. Here the number of folds is assigned to **2**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "lnwdZAvAZBBK"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "paramGrid = (ParamGridBuilder()\\\n",
        "              .addGrid(xgBoost.maxDepth, [2, 3, 9])\\\n",
        "              #.addGrid(xgBoost.maxBins, [1055,2000])\\\n",
        "              .addGrid(xgBoost.eta,[0.3, 0.7])\\\n",
        "              .build())\n",
        " #.addGrid(dt.maxBins, [1055,2000])\\  .addGrid(dt.maxBins, [2692,3000])\\\n",
        "#.addGrid(dt.maxBins, [2700,3000,4000])\\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "8zfH7aDkZBBL"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "#cv = CrossValidator(estimator=pipeline, evaluator= BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid, numFolds=2)\n",
        "\n",
        "cv = TrainValidationSplit(estimator=pipeline, evaluator=RegressionEvaluator(),  estimatorParamMaps=paramGrid, trainRatio=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "mMLxE2CgZBBL"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "from time import time\n",
        "from decimal import Decimal\n",
        "\n",
        "def with_benchmark(phrase, action):\n",
        "    start = time()\n",
        "    result = action()\n",
        "    end = time()\n",
        "    print('{} takes {} seconds'.format(phrase, (end - start))) #round(end - start, 2)))\n",
        "    return result    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "H48iJFkLZBBL"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "model = with_benchmark('training', lambda: cv.fit(train))\n",
        "#model =cv.fit(train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvixwA0qZBBL"
      },
      "source": [
        "\n",
        "### Test the Pipeline Model\n",
        "The model produced by the pipeline is a transformer that will apply all of the stages in the pipeline to a specified DataFrame and apply the trained model to generate predictions. In this case, we will transform the **test** DataFrame using the pipeline to generate label predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAE0Jm_KZBBL"
      },
      "source": [
        "### Save and Reload the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "oM8LE_wZZBBL"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "'''if IS_AWS == False:\n",
        "    model.write().overwrite().save('/data/new-model-path')\n",
        "    loaded_model = XGBoostClassificationModel().load('/data/new-model-path')'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Symy9J3KZBBL"
      },
      "source": [
        "\n",
        "## Transformation and Show Result Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "aq5GJt6GZBBL"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "\n",
        "\n",
        "def transform():\n",
        "    prediction = model.transform(test).cache()\n",
        "    prediction.foreachPartition(lambda _: None)\n",
        "    return prediction\n",
        "    \n",
        "prediction = with_benchmark('transform', transform)\n",
        "predicted = prediction.select(\"features\", \"prediction\", \"trueLabel\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "bfPKbJO8ZBBM"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "predicted.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "pFrbLQJIZBBM"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "evaluator = RegressionEvaluator()\\\n",
        "  .setLabelCol(\"trueLabel\")\\\n",
        "  .setPredictionCol(\"prediction\")\\\n",
        "  .setMetricName(\"rmse\")\n",
        " \n",
        "evaluator1 = RegressionEvaluator()\\\n",
        "  .setLabelCol(\"trueLabel\")\\\n",
        "  .setPredictionCol(\"prediction\")\\\n",
        "  .setMetricName(\"r2\")\n",
        "\n",
        "rmse = evaluator.evaluate(predicted)\n",
        "r2 = evaluator1.evaluate(predicted)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "T9h2YnESZBBM"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "print (\"RMSE = \", rmse, \" R2 = \", r2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLBfQuECZBBM"
      },
      "source": [
        "### airbnb_listing.csv (1.93GB) EMR 3 nodes: g4dn.2xlarge\n",
        "### GPU (S3)\n",
        "```\n",
        "|Data Size| Computing (sec)| Data Reading (sec)    | Data Eng   (sec)       | Train  Time (sec)       | Test  Time (sec)          | RMSE   | R2     |\n",
        "|1.93 GB  |                | 12.82,10.82,11.02,9.72| 24.10,25.80,25.49,15.84| 17.57,18.90,19.73,15.21 |0.9430,0.7229,0.9535,0.8147| 33.71  | 0.7300 |\n",
        "|1.92 GB  |  154           |                       |                        | 45.1                    |                           | 32.29  | 0.7524 |\n",
        "```\n",
        "### GPU (HDFS)\n",
        "```\n",
        "|Data Size | Data Reading (sec)    | Data Eng   (sec)       | Train  Time (sec)       | Test  Time (sec)   | RMSE           | R2             |\n",
        "|1.93 GB   |    5.72, 5.12         | 18.23, 21.336          | 15.53, 15.23            | 0.9955, 0.8092     | 33.35, 32.98   | 0.7300, 0.7368 |\n",
        "```\n",
        "\n",
        "### CPU (S3)\n",
        "``` \n",
        "|Data Sz|Comp (sec)| Data Reading (sec)       | Data Eng   (sec)             | Train  Time (sec)           | Test  Time (sec)            |RMSE | R2    |\n",
        "|1.93 GB|          |9.92,10.72, 9.62,9.52,9.52| 23.30,24.60,25.70,26.80,22.45|26.70,19.75,21.29,22.35,17.18|0.9384,,0.9958,0.9809,0.9949 |33.63| 0.7280|\n",
        "|1.92GB |  171     |                          |                              | 38.8                        |                             |31.90| 0.7503|\n",
        "\n",
        "``` \n",
        "\n",
        "### CPU (HDFS)\n",
        "``` \n",
        "|Data Sz | Data Reading (sec)   | Data Eng   (sec)   | Train  Time (sec)           | Test  Time (sec)  |RMSE         | R2            |\n",
        "|1.93 GB | 5.12, 5.12           | 18.93, 17.93       |17.63, 18.28                 |0.8181, 0.8390     |33.52, 33.51 | 0.7308, 0.7331|\n",
        "\n",
        "``` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "_PMqNdxdZBBM"
      },
      "outputs": [],
      "source": [
        "%sh\n",
        "\n",
        "nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yem7ButZBBM"
      },
      "source": [
        "References\n",
        "1. https://public.opendatasoft.com/explore/dataset/airbnb-listings/export/?disjunctive.host_verifications&disjunctive.amenities&disjunctive.features\n",
        "2. \n",
        "\n"
      ]
    }
  ]
}